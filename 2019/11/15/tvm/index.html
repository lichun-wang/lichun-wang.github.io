<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Spring's Idea" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="简介笔者也是最近偶然的机会才开始接触TVM，使用过后发现，经过auto-tuning后的TVM模型在速度是竟然超过了TensorRT,并且笔者使用的是MXNet模型，TVM对MXNet绝对的友好，对于Pytorch等模型，可以使用ONNX，操作一样简单，使用起来基本类似一键操作，本篇文章是笔者对TVM的简单整理，也算是对TVM的入门。">
<meta property="og:type" content="article">
<meta property="og:title" content="初识 TVM">
<meta property="og:url" content="https://www.wanglichun.tech/2019/11/15/tvm/index.html">
<meta property="og:site_name" content="Spring&#39;s Idea">
<meta property="og:description" content="简介笔者也是最近偶然的机会才开始接触TVM，使用过后发现，经过auto-tuning后的TVM模型在速度是竟然超过了TensorRT,并且笔者使用的是MXNet模型，TVM对MXNet绝对的友好，对于Pytorch等模型，可以使用ONNX，操作一样简单，使用起来基本类似一键操作，本篇文章是笔者对TVM的简单整理，也算是对TVM的入门。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/2.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/1.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/6.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/7.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/8.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/9.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/3.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/4.png">
<meta property="og:image" content="https://www.wanglichun.tech/2019/11/15/tvm/5.png">
<meta property="article:published_time" content="2019-11-15T00:44:39.000Z">
<meta property="article:modified_time" content="2022-05-28T01:33:22.268Z">
<meta property="article:author" content="Spring Wang">
<meta property="article:tag" content="模型加速">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.wanglichun.tech/2019/11/15/tvm/2.png">

<link rel="canonical" href="https://www.wanglichun.tech/2019/11/15/tvm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>初识 TVM | Spring's Idea</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f5aa4ee55174bece95c607a5becef769";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Spring's Idea</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2019/11/15/tvm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          初识 TVM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-15 08:44:39" itemprop="dateCreated datePublished" datetime="2019-11-15T08:44:39+08:00">2019-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-28 09:33:22" itemprop="dateModified" datetime="2022-05-28T09:33:22+08:00">2022-05-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Technology/" itemprop="url" rel="index">
                    <span itemprop="name">Technology</span>
                  </a>
                </span>
            </span>

          
            <span id="/2019/11/15/tvm/" class="post-meta-item leancloud_visitors" data-flag-title="初识 TVM" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/11/15/tvm/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/11/15/tvm/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>笔者也是最近偶然的机会才开始接触TVM，使用过后发现，经过auto-tuning后的TVM模型在速度是竟然超过了TensorRT,并且笔者使用的是MXNet模型，TVM对MXNet绝对的友好，对于Pytorch等模型，可以使用ONNX，操作一样简单，使用起来基本类似一键操作，本篇文章是笔者对TVM的简单整理，也算是对TVM的入门。</p>
<span id="more"></span>

<p>当然如何想详细了解TVM，还请阅读TVM的主页以及论文，文章最后有链接。</p>
<h4 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h4><p>随着深度学习的发展，深度学习的能力可以说是越来越强大，识别率节节攀升，甚至超过人类。于此同时，深度学习框架也变得越来越多，目前比较主流的深度学习框架包括：Pytorch、TensorFlow、Mxnet、Caffe、Keras等。</p>
<p>一般进行深度学习任务包括两部分，一是训练出精度比较高的模型，然后将其部署到对应的目标机器上。</p>
<p>针对第一部分，自然我们可以使用各种深度学习框架，通过修改网络调参等，训练出精度比较满意的模型，一般情况，在训练深度学习模型的时候，都会使用到GPU。</p>
<p>针对部署，这里的目标机包括服务器、手机、其他硬件设备等等。部署的模型自然是希望越快越好，所以硬件厂商一般会针对自己的硬件设备进行一定的优化，以使模型达到更高的效率，比如Nvidia的TensorRT。但是框架这么多，硬件平台这么多，并不是所有的硬件平台都像Nvidia提供了硬件加速库，而即使做了加速，要适应所有的深度学习训练框架，也是一件比较难的事情。</p>
<p>其实介绍了这么多总结起来就是两个问题：</p>
<ol>
<li>在进行模型部署的时候，我们是否可以对不同框架训练的模型均生成统一的模型，解决硬件平台需要适配所有框架的问题？</li>
<li>在进行模型部署的时候，我们是否可以自动化的针对不同的硬件进行优化，进而得到高效的模型？</li>
</ol>
<p><strong>TVM实际上就是在解决这两个问题，并且解决的还不错。</strong></p>
<p><strong>那么TVM是什么？</strong></p>
<blockquote>
<p>TVM is an open deep learning compiler stack for CPUs, GPUs, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends.</p>
</blockquote>
<blockquote>
<p>TVM是一个开源的可面向多种硬件设备的深度学习编译器，它的作用在于打通模型框架、模型表现以及硬件设备的鸿沟，进而得到表现最好的可部署的深度学习模型，实现端到端的深度学习模型部署。</p>
</blockquote>
<p><strong>TVM做了哪些工作</strong></p>
<p>针对第一个问题：</p>
<p>TVM将不同前端（深度学习框架）训练的模型，转换为统一的中间语言表示，如果想详细理解这里，可以了解一下<strong>NNVM</strong>，<strong>NNVM</strong>是陈天奇团队开发的可以针对不同框架进行深度学习编译的框架，在TVM中，陈天奇团队进一步优化，实现了<strong>NNVM</strong>的第二代<strong>Relay</strong>。Relay是TVM中实现的一种高级IR，可以简单理解为另一种计算图表示。其在TVM所处的位置如下图所示，并且该部分实现了比如<strong>运算融合</strong>等操作，可以提升一部分模型效率。</p>
<p><img src="/2019/11/15/tvm/2.png" alt="2.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Relay在优化中的位置</div>
</center>


<p>针对第二个问题：</p>
<p>TVM设计了对不同的硬件后端，自动优化tensor操作，以达到加速的目的。该部分的实现，TVM使用机器学习的方法进行计算空间的最优化搜索，通过在目标硬件上跑大量trial，来获得该硬件上相关运算（例如卷积）的最优实现。详细介绍可以参考TVM主页以及论文。</p>
<p><img src="/2019/11/15/tvm/1.png" alt="1.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">TVM tuning可以对不同硬件进行tensor优化</div>
</center>


<h4 id="TVM-安装"><a href="#TVM-安装" class="headerlink" title="TVM 安装"></a>TVM 安装</h4><p>不同环境的安装方法可以参考tvm的官网：<a target="_blank" rel="noopener" href="https://docs.tvm.ai/install/index.html">https://docs.tvm.ai/install/index.html</a></p>
<p>对于安装环境，我还是强烈推荐docker的,会少很多坑。</p>
<ul>
<li>直接pull陈天奇上传到dockerhub上的镜像，就可以tvm的各种操作了</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tvmai/demo-gpu:latest</span><br></pre></td></tr></table></figure>

<p>这个镜像是cuda8.0版本，如果需要在2080ti上实验，是跑不起来的，会报错。</p>
<ul>
<li>2080ti上重新build镜像</li>
</ul>
<p>（1）首先，把github的tvm项目拉到2080ti机器上。</p>
<p>（2）进入dockers文件夹，找到Dockerfile.demo_gpu，其内容默认是下面这样的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Minimum docker image for demo purposes</span><br><span class="line"># CI docker GPU env</span><br><span class="line"># tag: v0.54</span><br><span class="line">FROM tvmai/ci-gpu:v0.54</span><br><span class="line"></span><br><span class="line"># Jupyter notebook.</span><br><span class="line">RUN pip3 install matplotlib Image &quot;Pillow&lt;7&quot; jupyter[notebook]</span><br><span class="line"></span><br><span class="line"># Build TVM</span><br><span class="line">COPY install/install_tvm_gpu.sh /install/install_tvm_gpu.sh</span><br><span class="line">RUN bash /install/install_tvm_gpu.sh</span><br><span class="line"></span><br><span class="line"># Environment variables</span><br><span class="line">ENV PYTHONPATH=/usr/tvm/python:/usr/tvm/topi/python:/usr/tvm/vta/python:$&#123;PYTHONPATH&#125;</span><br><span class="line">ENV PATH=/usr/local/nvidia/bin:$&#123;PATH&#125;</span><br><span class="line">ENV PATH=/usr/local/cuda/bin:$&#123;PATH&#125;</span><br><span class="line">ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib64:$&#123;LD_LIBRARY_PATH&#125;</span><br></pre></td></tr></table></figure>
<p>（3）将FROM tvmai&#x2F;ci-gpu:v0.54修改为FROM tvmai&#x2F;ci-gpu:latest</p>
<p>（4）重新build这个Dockerfile就可以了。</p>
<h4 id="TVM-使用"><a href="#TVM-使用" class="headerlink" title="TVM 使用"></a>TVM 使用</h4><p>TVM的使用可以阅读一下tvm提供的tutorials：<a target="_blank" rel="noopener" href="https://docs.tvm.ai/tutorials/">https://docs.tvm.ai/tutorials/</a></p>
<p>主要推荐两部分：</p>
<ul>
<li>compile deep learning models</li>
<li>auto tuning</li>
</ul>
<p>其实简单的使用主要就是这两块内容，如果不想细研究其代码，可以将其当成一个工具使用，通过compile deep learning models,无论你使用什么样的框架，都可以生成统一的模型，一般会生成3个东西如下：</p>
<p><img src="/2019/11/15/tvm/6.png" alt="6.png"></p>
<p>这里一般会做一些层的融合等操作，速度会有一定的提升的，但是不是特别大。这时如果你需要进一步提速可以试试<strong>auto tuning</strong>,这部分可以参考tutorials以及下面的例子代码，auto-tune的时间一般比较长，但是效果还是比较显著的，本地测试，resnet在nvidia 1080ti上可以提高3倍左右。</p>
<h4 id="Demo代码"><a href="#Demo代码" class="headerlink" title="Demo代码"></a>Demo代码</h4><p>TVM的原理很复杂但是使用起来还是比较方便的，下面是使用MXNet进行TVM转换的demo。</p>
<p>代码一：生成TVM模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> relay</span><br><span class="line"><span class="keyword">from</span> tvm.relay <span class="keyword">import</span> testing</span><br><span class="line"><span class="keyword">from</span> tvm.contrib <span class="keyword">import</span> graph_runtime</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> tvm.contrib.download <span class="keyword">import</span> download_testdata</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">## load mxnet model</span></span><br><span class="line">prefix = <span class="string">&#x27;/Models/resnetv1d-101&#x27;</span></span><br><span class="line">epoch = <span class="number">13</span></span><br><span class="line">mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)</span><br><span class="line">shape_dict = &#123;<span class="string">&#x27;data&#x27;</span>: (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)&#125;</span><br><span class="line"></span><br><span class="line">relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict,</span><br><span class="line">        arg_params=arg_params, aux_params=aux_params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line"><span class="keyword">with</span> relay.build_config(opt_level=<span class="number">3</span>):</span><br><span class="line">    graph, lib, params = relay.build(relay_func, target, params=relay_params)</span><br><span class="line"><span class="comment"># run forward</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;test.jpg&#x27;</span>).resize((<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transform_image</span>(<span class="params">im</span>):</span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.transpose(im, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    im = im[np.newaxis, :]</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line">x = transform_image(image)</span><br><span class="line"><span class="comment"># let&#x27;s go</span></span><br><span class="line">ctx = tvm.gpu(<span class="number">0</span>)</span><br><span class="line">dtype = <span class="string">&#x27;float32&#x27;</span></span><br><span class="line"></span><br><span class="line">m = graph_runtime.create(graph, lib, ctx)</span><br><span class="line"><span class="comment">## set input data</span></span><br><span class="line">m.set_input(<span class="string">&#x27;data&#x27;</span>, tvm.nd.array(x.astype(dtype)))</span><br><span class="line"><span class="comment">## set input params</span></span><br><span class="line">m.set_input(**params)</span><br><span class="line">t1 = time.time()</span><br><span class="line">m.run()</span><br><span class="line">t2 = time.time()</span><br><span class="line"><span class="comment"># get output</span></span><br><span class="line">outputs = m.get_output(<span class="number">0</span>)</span><br><span class="line">top1 = np.argmax(outputs.asnumpy()[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(outputs, <span class="built_in">str</span>(t2-t1))</span><br><span class="line"></span><br><span class="line"><span class="comment">### evaluate inference time</span></span><br><span class="line"></span><br><span class="line">ftimer = m.module.time_evaluator(<span class="string">&#x27;run&#x27;</span>, ctx, number=<span class="number">1</span>, repeat=<span class="number">100</span>)</span><br><span class="line">prof_res = np.array(ftimer().results) * <span class="number">1000</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;time cost : mean:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.mean(prof_res)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># save model</span></span><br><span class="line"></span><br><span class="line">path_lib = <span class="string">&#x27;/Outputs/tvm/deploy_resnet101_v1d_lib.tar&#x27;</span></span><br><span class="line">lib.export_library(path_lib)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm/deploy_resnet101_v1d_graph.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(graph)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm/deploy_params&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(relay.save_param_dict(params))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># load model back</span></span><br><span class="line"></span><br><span class="line">loaded_json = <span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm/deploy_resnet101_v1d_graph.json&#x27;</span>).read()</span><br><span class="line">loaded_lib = tvm.module.load(path_lib)</span><br><span class="line">loaded_params = <span class="built_in">bytearray</span>(<span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm/deploy_params&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>).read())</span><br><span class="line">module = graph_runtime.create(loaded_json, loaded_lib, ctx)</span><br><span class="line">module.load_params(loaded_params)</span><br><span class="line"></span><br><span class="line">tvm_data = tvm.nd.array(x.astype(dtype))</span><br><span class="line">module.run(data=tvm_data)</span><br><span class="line">outputs = module.get_output(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>代码二：auto-tuning，这部分还是比较慢的，一个resnet101模型，在1080ti上面可能要tune1到2天的时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> autotvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> relay</span><br><span class="line"><span class="keyword">import</span> tvm.relay.testing</span><br><span class="line"><span class="keyword">from</span> tvm.autotvm.tuner <span class="keyword">import</span> XGBTuner, GATuner, RandomTuner, GridSearchTuner</span><br><span class="line"><span class="keyword">from</span> tvm.contrib.util <span class="keyword">import</span> tempdir</span><br><span class="line"><span class="keyword">import</span> tvm.contrib.graph_runtime <span class="keyword">as</span> runtime</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_network</span>(<span class="params">dtype, args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Get the symbol definition and random weight of a network&quot;&quot;&quot;</span></span><br><span class="line">    input_shape = (args.batch_size, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">    prefix = <span class="string">&#x27;/Models/&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.version, args.model_name)</span><br><span class="line">    epoch = args.model_index</span><br><span class="line">    mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)</span><br><span class="line"></span><br><span class="line">    mod, params = relay.frontend.from_mxnet(mx_sym, shape=&#123;<span class="string">&#x27;data&#x27;</span>: input_shape&#125;, dtype=dtype, arg_params=arg_params,</span><br><span class="line">                                            aux_params=aux_params)</span><br><span class="line">    net = mod[<span class="string">&quot;main&quot;</span>]</span><br><span class="line">    net = relay.Function(net.params, relay.nn.softmax(net.body), <span class="literal">None</span>, net.type_params, net.attrs)</span><br><span class="line">    mod = relay.Module.from_expr(net)</span><br><span class="line">    <span class="keyword">return</span> mod, params, input_shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can skip the implementation of this function for this tutorial.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tune_tasks</span>(<span class="params">tasks,</span></span><br><span class="line"><span class="params">               measure_option,</span></span><br><span class="line"><span class="params">               tuner=<span class="string">&#x27;xgb&#x27;</span>,</span></span><br><span class="line"><span class="params">               n_trial=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">               early_stopping=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               log_filename=<span class="string">&#x27;tuning.log&#x27;</span>,</span></span><br><span class="line"><span class="params">               use_transfer_learning=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">               try_winograd=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">if</span> try_winograd:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tasks)):</span><br><span class="line">            <span class="keyword">try</span>:  <span class="comment"># try winograd template</span></span><br><span class="line">                tsk = autotvm.task.create(tasks[i].name, tasks[i].args,</span><br><span class="line">                                          tasks[i].target, tasks[i].target_host, <span class="string">&#x27;winograd&#x27;</span>)</span><br><span class="line">                input_channel = tsk.workload[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> input_channel &gt;= <span class="number">64</span>:</span><br><span class="line">                    tasks[i] = tsk</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># create tmp log file</span></span><br><span class="line">    tmp_log_file = log_filename + <span class="string">&quot;.tmp&quot;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(tmp_log_file):</span><br><span class="line">        os.remove(tmp_log_file)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, tsk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(tasks)):</span><br><span class="line">        prefix = <span class="string">&quot;[Task %2d/%2d] &quot;</span> %(i+<span class="number">1</span>, <span class="built_in">len</span>(tasks))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create tuner</span></span><br><span class="line">        <span class="keyword">if</span> tuner == <span class="string">&#x27;xgb&#x27;</span> <span class="keyword">or</span> tuner == <span class="string">&#x27;xgb-rank&#x27;</span>:</span><br><span class="line">            tuner_obj = XGBTuner(tsk, loss_type=<span class="string">&#x27;rank&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> tuner == <span class="string">&#x27;ga&#x27;</span>:</span><br><span class="line">            tuner_obj = GATuner(tsk, pop_size=<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">elif</span> tuner == <span class="string">&#x27;random&#x27;</span>:</span><br><span class="line">            tuner_obj = RandomTuner(tsk)</span><br><span class="line">        <span class="keyword">elif</span> tuner == <span class="string">&#x27;gridsearch&#x27;</span>:</span><br><span class="line">            tuner_obj = GridSearchTuner(tsk)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid tuner: &quot;</span> + tuner)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_transfer_learning:</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(tmp_log_file):</span><br><span class="line">                tuner_obj.load_history(autotvm.record.load_from_file(tmp_log_file))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># do tuning</span></span><br><span class="line">        n_trial = <span class="built_in">min</span>(n_trial, <span class="built_in">len</span>(tsk.config_space))</span><br><span class="line">        tuner_obj.tune(n_trial=n_trial,</span><br><span class="line">                       early_stopping=early_stopping,</span><br><span class="line">                       measure_option=measure_option,</span><br><span class="line">                       callbacks=[</span><br><span class="line">                           autotvm.callback.progress_bar(n_trial, prefix=prefix),</span><br><span class="line">                           autotvm.callback.log_to_file(tmp_log_file)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># pick best records to a cache file</span></span><br><span class="line">    autotvm.record.pick_best(tmp_log_file, log_filename)</span><br><span class="line">    os.remove(tmp_log_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tune_and_evaluate</span>(<span class="params">tuning_opt, target, log_file, dtype, args</span>):</span><br><span class="line">    <span class="comment"># extract workloads from relay program</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Extract tasks...&quot;</span>)</span><br><span class="line">    mod, params, input_shape = get_network(dtype, args)</span><br><span class="line">    tasks = autotvm.task.extract_from_program(mod[<span class="string">&quot;main&quot;</span>], target=target,</span><br><span class="line">                                              params=params, ops=(relay.op.nn.conv2d,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># run tuning tasks</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Tuning...&quot;</span>)</span><br><span class="line">    tune_tasks(tasks, **tuning_opt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compile kernels with history best records</span></span><br><span class="line">    <span class="keyword">with</span> autotvm.apply_history_best(log_file):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Compile...&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> relay.build_config(opt_level=<span class="number">3</span>):</span><br><span class="line">            graph, lib, params = relay.build_module.build(</span><br><span class="line">                mod, target=target, params=params)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># export library</span></span><br><span class="line">        tmp = tempdir()</span><br><span class="line">        filename = <span class="string">&quot;/Outputs/tvm_autotuning/&#123;&#125;/&#123;&#125;_auto_tune_deploy_batch_&#123;&#125;_lib.tar&quot;</span>.<span class="built_in">format</span>(args.version,args.model_name, args.batch_size)</span><br><span class="line">        lib.export_library(tmp.relpath(filename))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm_autotuning/&#123;&#125;/&#123;&#125;_auto_tune_deploy_batch_&#123;&#125;_graph.json&#x27;</span>.<span class="built_in">format</span>(args.version,args.model_name,args.batch_size) , <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(graph)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/Outputs/tvm_autotuning/&#123;&#125;/&#123;&#125;_auto_tune_deploy_batch_&#123;&#125;_params.params&#x27;</span>.<span class="built_in">format</span>(args.version,args.model_name,args.batch_size) , <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(relay.save_param_dict(params))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load parameters</span></span><br><span class="line">        ctx = tvm.context(<span class="built_in">str</span>(target), <span class="number">0</span>)</span><br><span class="line">        module = runtime.create(graph, lib, ctx)</span><br><span class="line">        data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))</span><br><span class="line">        module.set_input(<span class="string">&#x27;data&#x27;</span>, data_tvm)</span><br><span class="line">        module.set_input(**params)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># evaluate</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Evaluate inference time cost...&quot;</span>)</span><br><span class="line">        ftimer = module.module.time_evaluator(<span class="string">&quot;run&quot;</span>, ctx, number=<span class="number">1</span>, repeat=<span class="number">600</span>)</span><br><span class="line">        prof_res = np.array(ftimer().results) * <span class="number">1000</span>  <span class="comment"># convert to millisecond</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Mean inference time (std dev): %.2f ms (%.2f ms)&quot;</span> %</span><br><span class="line">              (np.mean(prof_res), np.std(prof_res)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># We do not run the tuning in our webpage server since it takes too long.</span></span><br><span class="line"><span class="comment"># Uncomment the following line to run it by yourself.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&#x27;score a model on a dataset&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--version&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;porno&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model-name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;resnetv1d-101-320x320&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model-index&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--tag&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.join(<span class="string">&#x27;/Outputs/tvm_autotuning/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.version))):</span><br><span class="line">        os.mkdir(os.path.join(<span class="string">&#x27;/Outputs/tvm_autotuning/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(args.version)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#### DEVICE CONFIG ####</span></span><br><span class="line">    target = tvm.target.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#### TUNING OPTION ####</span></span><br><span class="line">    log_file = <span class="string">&#x27;/Outputs/tvm_autotuning/&#123;&#125;/&#123;&#125;_batch_&#123;&#125;.log&#x27;</span>.<span class="built_in">format</span>(args.version, args.model_name, args.batch_size)</span><br><span class="line">    dtype = <span class="string">&#x27;float32&#x27;</span></span><br><span class="line"></span><br><span class="line">    tuning_option = &#123;</span><br><span class="line">        <span class="string">&#x27;log_filename&#x27;</span>: log_file,</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;tuner&#x27;</span>: <span class="string">&#x27;xgb&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;n_trial&#x27;</span>: <span class="number">2000</span>,</span><br><span class="line">        <span class="string">&#x27;early_stopping&#x27;</span>: <span class="number">600</span>,</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;measure_option&#x27;</span>: autotvm.measure_option(</span><br><span class="line">            builder=autotvm.LocalBuilder(timeout=<span class="number">10</span>),</span><br><span class="line">            runner=autotvm.LocalRunner(number=<span class="number">20</span>, repeat=<span class="number">3</span>, timeout=<span class="number">4</span>, min_repeat_ms=<span class="number">150</span>),</span><br><span class="line">            <span class="comment"># runner=autotvm.RPCRunner(</span></span><br><span class="line">            <span class="comment">#     &#x27;1080ti&#x27;,  # change the device key to your key</span></span><br><span class="line">            <span class="comment">#     &#x27;0.0.0.0&#x27;, 9190,</span></span><br><span class="line">            <span class="comment">#     number=20, repeat=3, timeout=4, min_repeat_ms=150)</span></span><br><span class="line">        ),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tune_and_evaluate(tuning_option, target, log_file, dtype, args)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><ul>
<li>本地Nvidia 1080ti 测试</li>
</ul>
<p>首先在，两卡机上测试性能，分别测试了resnet50v1d，resnet101v1d以及输入尺度320的resnet101v1d，测试结果如下表：</p>
<p>可以发现，相比较mxnet模型  tensorRT大约加速比70%-80%，TVM提速可以达到50%-60%</p>
<p><img src="/2019/11/15/tvm/7.png" alt="7.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Nvidia 1080 测试结果</div>
</center>

<ul>
<li>Nvidia 2080ti 测试</li>
</ul>
<p>如下分别测试了tvm加速在2080上的效果，可以发现：</p>
<ul>
<li>在2080机器上，mxnet的提速还是比较明显的，加速比大概 70%</li>
<li>在2080机器上，tensorRT float32速度，跟在1080上基本一致，没有提速。</li>
<li>在2080机器上，TVM速度与在1080上相比，反而有一丢丢的减速。</li>
</ul>
<p>在2080机器上重新auto-tune了TVM模型，可以发现：</p>
<ul>
<li>重新在2080机器上tune后，相比不tune，效果的提升还是比较明显的。</li>
<li>重新在2080机器上tune后，跟在1080上的速度相差不大。</li>
</ul>
<p><img src="/2019/11/15/tvm/8.png" alt="8.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Nvidia 2080 测试结果</div>
</center>

<p>下面测试了内存的消耗。总体来看TVM比较省内存和显存。</p>
<p><img src="/2019/11/15/tvm/9.png" alt="9.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">内存显存消耗</div>
</center>



<h4 id="补充：TensorRT"><a href="#补充：TensorRT" class="headerlink" title="补充：TensorRT"></a>补充：TensorRT</h4><p>TensorRT是Nvidia出品的用于将不同框架训练的模型部署到GPU的加速引擎，可以自动将不同框架的模型转换为TensorRT模型，并进行模型加速。</p>
<p>TensorRT进行模型加速主要有两点：</p>
<ul>
<li>TensorRT支持int8以及FP16计算</li>
<li>TensorRT对网络进行重构以及优化:</li>
</ul>
<blockquote>
<p>去掉网络中的无用层</p>
</blockquote>
<blockquote>
<p>网络结构的垂直整合</p>
</blockquote>
<blockquote>
<p>网络结构的水平融合</p>
</blockquote>
<p><img src="/2019/11/15/tvm/3.png" alt="3.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">原始网络</div>
</center>


<p><img src="/2019/11/15/tvm/4.png" alt="4.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">纵向融合</div>
</center>


<p><img src="/2019/11/15/tvm/5.png" alt="5.png"></p>
<center>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">横向融合</div>
</center>


<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a target="_blank" rel="noopener" href="https://tvm.ai/">TVM官网： https://tvm.ai/</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.04799">TVM论文：arxiv: https://arxiv.org/abs/1802.04799</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xh_hit/article/details/79769599">tensorRT加速参考文献：https://blog.csdn.net/xh_hit&#x2F;article&#x2F;details&#x2F;79769599</a></p>
<p><a target="_blank" rel="noopener" href="https://devblogs.nvidia.com/production-deep-learning-nvidia-gpu-inference-engine/">Nvidia参考文献：https://devblogs.nvidia.com/production-deep-learning-nvidia-gpu-inference-engine/</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/" rel="tag"># 模型加速</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2019/11/12/dpn/" rel="next" title="DPN">
                  <i class="fa fa-chevron-left"></i> DPN
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/2019/11/18/refinedet/" rel="prev" title="RefineDet">
                  RefineDet <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TVM-%E5%AE%89%E8%A3%85"><span class="nav-number">3.</span> <span class="nav-text">TVM 安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TVM-%E4%BD%BF%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">TVM 使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Demo%E4%BB%A3%E7%A0%81"><span class="nav-number">5.</span> <span class="nav-text">Demo代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-number">6.</span> <span class="nav-text">测试结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9ATensorRT"><span class="nav-number">7.</span> <span class="nav-text">补充：TensorRT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">8.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Spring Wang"
    src="/images/head.png">
  <p class="site-author-name" itemprop="name">Spring Wang</p>
  <div class="site-description" itemprop="description">众里寻他千百度，蓦然回首，那人却在灯火阑珊处</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lichun-wang" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;lichun-wang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/lichun_wang1993@163.com" title="E-Mail &amp;rarr; lichun_wang1993@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Chunfengyanyulove" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;Chunfengyanyulove" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/chunfengyanyu" title="Weibo &amp;rarr; https:&#x2F;&#x2F;weibo.com&#x2F;chunfengyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Spring Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v6.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        






  <script>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz',
            'X-LC-Key': 'VAwpszUEpH7osEG3O76jh3be',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>






        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz',
    appKey: 'VAwpszUEpH7osEG3O76jh3be',
    placeholder: "comments",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
