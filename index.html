<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Spring's Idea" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
<meta property="og:type" content="website">
<meta property="og:title" content="Spring&#39;s Idea">
<meta property="og:url" content="https://www.wanglichun.tech/index.html">
<meta property="og:site_name" content="Spring&#39;s Idea">
<meta property="og:description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Spring Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www.wanglichun.tech/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Spring's Idea</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f5aa4ee55174bece95c607a5becef769";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Spring's Idea</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/05/27/bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/27/bert/" class="post-title-link" itemprop="url">bert</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-27 22:14:10" itemprop="dateCreated datePublished" datetime="2022-05-27T22:14:10+08:00">2022-05-27</time>
            </span>

          
            <span id="/2022/05/27/bert/" class="post-meta-item leancloud_visitors" data-flag-title="bert" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/05/27/bert/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/05/27/bert/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body></body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/27/bert/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/05/27/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/27/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-27 13:52:46" itemprop="dateCreated datePublished" datetime="2022-05-27T13:52:46+08:00">2022-05-27</time>
            </span>

          
            <span id="/2022/05/27/hello-world/" class="post-meta-item leancloud_visitors" data-flag-title="Hello World" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/05/27/hello-world/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/05/27/hello-world/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/27/hello-world/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/05/27/resnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/27/resnet/" class="post-title-link" itemprop="url">ResNet(Deep Residual Learning for Image Recognition)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-05-27 13:14:50 / 修改时间：23:12:28" itemprop="dateCreated datePublished" datetime="2022-05-27T13:14:50+08:00">2022-05-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>
            </span>

          
            <span id="/2022/05/27/resnet/" class="post-meta-item leancloud_visitors" data-flag-title="ResNet(Deep Residual Learning for Image Recognition)" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/05/27/resnet/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/05/27/resnet/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Resnet是2015年ImageNet比赛的冠军，不仅在分类上标线优秀，在目标检测中同样取得好成绩，Resnet将网络层数进一步加深，甚至达到1000+层。ResNet的表现以至于后面的网络都是在其基础上进行修改得到的，可以说ResNet是一个划时代的网络，被广泛应用于工业界。</p>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/27/resnet/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/05/27/tricks-of-train/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/27/tricks-of-train/" class="post-title-link" itemprop="url">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-27 13:14:50" itemprop="dateCreated datePublished" datetime="2022-05-27T13:14:50+08:00">2022-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-16 03:31:28" itemprop="dateModified" datetime="2020-02-16T03:31:28+08:00">2020-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>
            </span>

          
            <span id="/2022/05/27/tricks-of-train/" class="post-meta-item leancloud_visitors" data-flag-title="Bag of Tricks for Image Classification with Convolutional Neural Networks" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/05/27/tricks-of-train/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/05/27/tricks-of-train/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><blockquote>
<p>很多时候，外界同学管深度学习算法工程师叫做<strong>调参工程师</strong>，简单直接的概括了深度学习工程师的工作，搞深度学习的同学自己也经常自嘲，称自己的工作是<strong>炼丹</strong>，的确，深度学习模型有时候确实很奇妙，而<strong>调参</strong>在一个模型的优化中起着至关重要的作用，正因为如此，也有越来越多的研究放在了<strong>调参</strong>这件事上，比如：学习率的优化算法，模型初始化算法等等。<br>其实，拿一个别人已经训练好的模型（比如ImageNet上预训练的ResNet），直接在自己的数据集上进行finetune，不需要怎么调参，一般都会得到不错的效果，这就是站在巨人的肩膀上，但是如果想继续提高模型的精度，该怎么做？继续调参？还是有一些其他的方法可以采用？<strong>本篇文章就介绍了Amazon工程师总结的分类模型的调参技巧。</strong></p>
</blockquote>
<hr>
<p><strong>论文名称：Bag of Tricks for Image Classification with Convolutional Neural Networks</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.01187.pdf">论文链接：https://arxiv.org/pdf/1812.01187.pdf</a></strong></p>
<hr>
<p><strong>下面直接上重点，正文开始</strong></p>
<h4 id="参数的优化方法"><a href="#参数的优化方法" class="headerlink" title="参数的优化方法"></a><strong>参数的优化方法</strong></h4><p>证明方法有效的最直接方法就是跟其他方法的效果最做对比，要对比当然就需要有一个baseline，这里就利用最常用的深度学习模型训练方法先训练一个base model，如下：</p>
<hr>
<ul>
<li><strong>训练数据预处理</strong>：</li>
</ul>
<ol>
<li>随机旋转一个batch的图像，然后将其编码成32位浮点数[0-255]。</li>
<li>随机截取一个长宽比在[3/4,4/3]的矩形，矩形面积占图像面积的[0.8,1],截取后将图像resize到224*224。</li>
<li>按照0.5的比例进行水平翻转。</li>
<li>对亮度，色度，饱和度进行跳帧。</li>
<li>增加PCA噪声，噪声分布为正态分布(0,0.1)。</li>
<li>对图像像素，减均值[123.68,116.779,103.939]，除标准差[58.393,57.12,57.375]。</li>
</ol>
<ul>
<li><strong>验证数据</strong>：</li>
</ul>
<ol start="7">
<li>短边缩放到256</li>
<li>中间截取224*224</li>
<li>对图像像素，减均值[123.68,116.779,103.939]，除标准差[58.393,57.12,57.375]</li>
</ol>
<ul>
<li><strong>参数初始化</strong></li>
</ul>
<ol start="10">
<li>卷积层以及全连接层采用Xavier算法进行初始化。</li>
<li>bn层，$\gamma=1$,$\beta=0$</li>
</ol>
<ul>
<li><strong>参数优化方法</strong></li>
</ul>
<ol start="12">
<li>梯度采用带动量的梯度优化方法：Nesterov Accelerated Gradient</li>
<li>学习率：初始学习率0.1，每30个epoch学习率下降为原来的10%</li>
<li>batchsize:256</li>
</ol>
<hr>
<p><strong>上面是最常用的深度学习模型的参数设置，作者将其作为模型的baseline，基础有了，下面我们来谈谈如何提升模型效果：</strong></p>
<h5 id="batch-size是不是越大越好？"><a href="#batch-size是不是越大越好？" class="headerlink" title="batch size是不是越大越好？"></a><strong>batch size是不是越大越好？</strong></h5><p>增加batch size可以增加网络的并行度，降低通信消耗，但是使用大的batch size同样也会带来一定的问题，比如：凸优化问题，<strong>随着batch size的增加，会增加数据的收敛的难度</strong>，换句话说，相同的epoch，使用大的batch size相比较使用小的batch size，小的batch size可能精度会更高一点。</p>
<p><strong>那么我们是不是不该使用大的batch size，当然不是，下面介绍几种方法:</strong></p>
<h5 id="方法一：线性改变学习率"><a href="#方法一：线性改变学习率" class="headerlink" title="方法一：线性改变学习率"></a><strong>方法一：线性改变学习率</strong></h5><p>在梯度下降中，由于选择的sample是随机的，所以其梯度下降的方向也是随机的，当提高batch size之后，并不能改变这种随机性，但是由于图像数量的增加，却可以中和掉一部分的噪声，所以这个时候，我们可以增加一部分的学习率，使得学习的步子迈的大一点，比如，在resnet50中，batch size=256,我们选择了lr=0.1，当batchsize增大到b的时候，lr可以调整为$0.1b/256$</p>
<h5 id="方法二：学习率预热"><a href="#方法二：学习率预热" class="headerlink" title="方法二：学习率预热"></a><strong>方法二：学习率预热</strong></h5><p>当我们开始训练模型的时候，往往模型的参数都是随机初始化的，并不能代表什么，所以如果此时选择一个较大的学习率，往往会导致模型的不稳定，那么什么是学习率预热，简单来说就是先使用一个较小的学习率，先迭代几个epoch，等到模型基本稳定的时候再用初始设置的学习率进行训练。举个例子，比如预热5个epoch，学习率设置成lr，则前5个epoch可以设置学习率线性递增，即第一个epoch：0.2*lr,第二个:0.4*epoch,依次类推，到第五个变为lr。</p>
<h5 id="方法三：部分BN层r-gamma-设置成0"><a href="#方法三：部分BN层r-gamma-设置成0" class="headerlink" title="方法三：部分BN层r(gamma)设置成0"></a><strong>方法三：部分BN层r(gamma)设置成0</strong></h5><p>resnet我们都知道，中间有很多BN层，BN层的提出可以说是模型训练的一个里程碑，它使得模型的训练更加简单，模型收敛更加快速，并且可以使用更大的学习率进行训练，BN层的作用就是对数据进行归一化操作，然后通过设置两个学习参数对归一化进行调整，即：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=y=\gamma&amp;space;\widehat{x}+\beta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=\gamma&amp;space;\widehat{x}+\beta" title="y=\gamma \widehat{x}+\beta"></a></p>
<p>其中$\gamma$以及$\beta$是可以学习的，$\widehat{x}$是对输入进行标准化的结果，通常的做法是将$\gamma$初始化为1，$\beta$初始化为0，这里作者建议在使用resnet的时候，将每个block的最后的BN层的$\gamma$初始化为0，这样无论前面的结果如何，经过这一层都被清零了，block的输出就只有前面的short cut部分，导致每个block的输入都是一样的，作者解释说，这样可以使得网络的训练更加的方便。</p>
<h5 id="方法四：不使用bias-decay"><a href="#方法四：不使用bias-decay" class="headerlink" title="方法四：不使用bias decay"></a><strong>方法四：不使用bias decay</strong></h5><p>在深度学习训练中，decay是一个很好的策略，可以防止参数多大引起过拟合，一般常采用的策略是L2范数，这里作者建议在做decay的时候，只对卷基层以及全连接层的weight加入decay就可以了，不需要对bias进行处理。</p>
<h5 id="方法五：低精度训练"><a href="#方法五：低精度训练" class="headerlink" title="方法五：低精度训练"></a><strong>方法五：低精度训练</strong></h5><p>目前的GPU训练基本是采用32位浮点数进行数据存储的，即FP32,但是新的GPU比如Nvidia V100,支持16为浮点数的运行，速度可以提升2-3倍，而且采用更大的batchsize后（当然使用了前面的各种策略来配合batchsize的提升），精度还小有提升。如下面Table3所示。</p>
<p><img src="/1.PNG"></p>
<p>当然，哪步更有用作者同样给了分析，如下表Table 4，可见，Zero $\gamma$带来的精度提升是最大的。</p>
<p><img src="/2.PNG"></p>
<h4 id="网络的优化方法"><a href="#网络的优化方法" class="headerlink" title="网络的优化方法"></a><strong>网络的优化方法</strong></h4><p><img src="/3.PNG"></p>
<p>对于resnet结构，我们应该都不陌生，如果不了解可以查看博客:<a target="_blank" rel="noopener" href="https://blog.csdn.net/Chunfengyanyulove/article/details/79253656">ResNet(Deep Residual Learning for Image Recognition)</a><br>在网络设计中，设计模型结构是最难的，下面介绍几个比较成熟的技巧：</p>
<ul>
<li>ResNet B:</li>
</ul>
<p>由于在ResNet中，每个stage的第一个block会进行图像尺度的缩小，采用了如图Figure1 深蓝色那部分结构，首先是1*1卷积层，stride=2，使得网络的feature map缩小一半，然后再经过3*3的卷积层，但是stride=2的1*1的卷积层会带来一个问题，会损失掉一半的信息（这里论文中提到是3/4），ResNet B修改了这个结构，将stride=2放到了3*3的卷积，这样就不会带来信息的损失了。</p>
<ul>
<li>ResNet C:</li>
</ul>
<p>使用3个3*3的卷积来替换resnet的7*7的卷积。并且前2个卷积的stride=3,channel=32,最后一个channel=64，这是一个比较老的套路了，在inception网络中被提出。</p>
<ul>
<li>ResNet D:</li>
</ul>
<p>既然主通道可以通过修改stride来降低信息损失，那么short cut为什么不可以呢？当然可以，作者是怎么做的呢？首先增加了一个2*2的average pooling layer， 设置stride=2，conv层的stride变为1。</p>
<p><strong>对比一下这几个结构的精度,resnet B的提升是最有效的，相对提升了约0.5个百分点resnet C以及resnet D分别又提升了0.2%以及0.3%，整体大约提升了1个百分点。</strong></p>
<p><img src="/4.PNG"></p>
<h4 id="模型的训练技巧"><a href="#模型的训练技巧" class="headerlink" title="模型的训练技巧"></a><strong>模型的训练技巧</strong></h4><h5 id="技巧一-cosine-learning-rate-decay："><a href="#技巧一-cosine-learning-rate-decay：" class="headerlink" title="技巧一  : cosine learning rate decay："></a>技巧一  : cosine learning rate decay：</h5><p>前面我们提到了训练模型的时候学习率需要warmup，可是在warmup之后，随着epoch的增加，学习率需要适度的调低，这就叫learning rate decay，我们常采用的方法是使用<strong>step decay</strong>,最简单的比如每20个epoch降低学习率为原来的10%，本篇综述提到了使用cosine learning rate decay，即采用cosine的方式来降低学习率，公式如下：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\eta&amp;space;_{n}=\frac{1}{2}(1+cos(\frac{t\pi&amp;space;}{T}))\eta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\eta&amp;space;_{n}=\frac{1}{2}(1+cos(\frac{t\pi&amp;space;}{T}))\eta" title="\eta _{n}=\frac{1}{2}(1+cos(\frac{t\pi }{T}))\eta"></a></p>
<p>红色的线为采用cosine decay策略，蓝色的为采用step decay策略，可以发现，cosine decay策略更加的平滑，训练的精度提升也是逐步提升，不像step策略，会有跳跃。不过训到最后，精度基本差不多，<strong>个人觉得，step其实也挺好用。</strong></p>
<p><img src="/5.PNG"></p>
<h5 id="技巧二-Label-Smoothing"><a href="#技巧二-Label-Smoothing" class="headerlink" title="技巧二 : Label Smoothing"></a>技巧二 : Label Smoothing</h5><p>在分类算法中，我们常采用的是one-hot编码，label smoothing的策略就是在one-hot的基础上，减去一个较小的值，如下公式，作者解释到，<strong>这样可以一定程度上减少过拟合</strong>，在采用one-hot编码的时候，只需要计算label类别的损失就可以了，采用label smoothing后，不仅仅需要计算label类别的损失，还需要增加其他类别的损失，这样，在one-hot编码的时候，对应目标的输出的目标值是正无穷，这样跟其他类别的差距更大，而增加了smooth label之后，由于引入了参数$\sigma$，所以随着$\sigma$的变化，其目标发生了变化，作者也画出了其目标图，如图figure 4(a)，可见Gap基本集中在9左右，实际的实验结果如图Figure 4(b)，也证实了这一点，b也符合a中基本都集中在9左右，并且b中smooth明显要比one-hot要小一点。</p>
<p><img src="/6.PNG"></p>
<h5 id="技巧三：知识蒸馏"><a href="#技巧三：知识蒸馏" class="headerlink" title="技巧三：知识蒸馏"></a>技巧三：知识蒸馏</h5><p>知识蒸馏也是提升模型精度的一个方法，知识蒸馏中，一般有一个精度较好的model作为teacher model，利用teacher model去帮助student model训练，比如：可以采用resnet-152作为teacher model，resnet-50作为student model。在利用知识蒸馏的方法进行训练的时候，需要增加用于蒸馏的loss,举个例子，假设p是真实概率，z和r分别是student以及teacher model的全连接层的输出结果，则损失函数为：</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=l=(p,softmax(z))+T^{2}l(softmax(r/T),softmax(z/T))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l=(p,softmax(z))+T^{2}l(softmax(r/T),softmax(z/T))" title="l=(p,softmax(z))+T^{2}l(softmax(r/T),softmax(z/T))"></a></p>
<p>这里解释一下这个T(蒸馏温度参数)，T是一个使得softmax output更加平滑的参数，以便于student model从teacher model学习参数。</p>
<p>下图是设置不同的T(蒸馏温度)得到的值，可以看到随着T的增大，曲线便的越来越平滑，其实设置这个标签的目的就是软化标签，增加训练难度，这样在inference的时候，将T重新设置为1，有难度的时候都可以表现很好，简单模式下，这样其分类的准确性就会更高了。</p>
<p><img src="/7.PNG"></p>
<h5 id="技巧四：mixup-training"><a href="#技巧四：mixup-training" class="headerlink" title="技巧四：mixup training"></a>技巧四：mixup training</h5><p>所谓的mixup training，就是每次要取出2张图像，然后将两张图像进行线性组合，得到新的图像，以此来作为新的训练样本，进行网络的训练，如下公式，其中x代表图像数据，y代表标签，则得到的$\widehat{x}$,$\widehat{y}$则为送入网络的训练样本。</p>
<p><a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{x}=\lambda&amp;space;x_i+(1-\lambda&amp;space;)x_j&amp;space;,&amp;space;\widehat{y}=\lambda&amp;space;y_i+(1-\lambda&amp;space;)y_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{x}=\lambda&amp;space;x_i+(1-\lambda&amp;space;)x_j&amp;space;,&amp;space;\widehat{y}=\lambda&amp;space;y_i+(1-\lambda&amp;space;)y_j" title="\widehat{x}=\lambda x_i+(1-\lambda )x_j , \widehat{y}=\lambda y_i+(1-\lambda )y_j"></a></p>
<p>mixup方法主要增强了训练样本之间的线性表达，增强网络的泛化能力，并且使用mixup方法需要较长的时间收敛。</p>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>Table 6是作者使用上面不同的方法进行的实验结果，其中w/代表的是with,w/o代表的是without，根据实验结果可以发现，在使用了cosine decay，label smooth方法，在ImageNet的结果上，基本会提高一个点左右。采用mixup方法，三个网络也基本一致的提升了，对于蒸馏的效果，resnet网络的效果提升了大约0.3%，但是对于Inception-V3以及MobileNet，精度都下降了，为什么会出现这种情况呢？原因可能是：由于这里是利用ResNet-152作为teacher model的，而ResNet-152的输出的数据分布和Inception以及mobileNet的分布不同，所以导致了结果的不一致性。</p>
<p>Table 7是作者在Places 365数据集的测试结果，结果表明，采用这几个策略进行训练的结果同样也是有效的。</p>
<p><img src="/8.PNG"></p>
<p><strong>既然我们的模型在分类任务上表现提升了，那么使用此模型，在目标检测以及目标分割上是否有用呢？</strong></p>
<p>首先作者测试了其在Faster R-CNN中的效果，测试结果如下表所示，这里，作者使用的是VGG-19作为backbone，使用不同精度的预训练模型进行训练，可以发现，在使用了精度更高的预训练模型之后，Faster R-CNN的mAP最终提高了大约4%（77.54-&gt;81.33）</p>
<p><img src="/9.PNG"></p>
<p>在图像分割方面，最具代表性的网络就是FCN，作者在FCN网络上测试了不同精度的backbone对FCN的影响，结果如下表所示，可见，采用了作者调优后的模型还是具有一定的效果的，不过这里对于采用了cosine优化的效果最佳，对于采用了label smoothing，mixup等方法效果不是特别的明显，这是为什么呢？<strong>猜想应该是图像分割是对每个像素进行分类，而采用了诸如label smoothing方法，本身对于像素的标签产生了一定的影响，采用mixup等方法直接对像素值进行了改变，进而影响了对像素分类的效果</strong>。 </p>
<p><img src="/10.PNG"></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本篇文章是Amazon对于分类模型炼丹方法的一个总结，介绍了很多trick，还是有很多借鉴意义的，至少我现在使用的其提供的resnet模型，在分类效果上确实有一定的提升。</p>
<h4 id="彩弹"><a href="#彩弹" class="headerlink" title="彩弹"></a>彩弹</h4><p>对于Amazon这样的大公司，当然也不会只是纸上谈兵，既然讲了这么多的方法，有没有预训练模型提供给我们使用呢？当然是有的，不过由于亚马逊推的是自己的深度学习框架MXNet+Gluon,所以这些预训练模型是在最新的GluonCV的model_zoo中提供。</p>
<p>有兴趣的读者可以自行查看，网站链接如下：</p>
<p><a target="_blank" rel="noopener" href="https://gluon-cv.mxnet.io/model_zoo/classification.html">https://gluon-cv.mxnet.io/model_zoo/classification.html</a></p>
<p>并且，gluoncv中还给出了各个模型的运行时间对比，内存消耗对比等，如下图所示，方便大家根据自己的需求选择合适的模型。</p>
<p><img src="http://ww1.sinaimg.cn/large/87675bbbly1fzlf1xibwij20is0fhdh5.jpg"></p>
<p>最后附上一段使用gluoncv进行imagenet分类的代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> gluoncv</span><br><span class="line"></span><br><span class="line"><span class="comment"># you can change it to your image filename</span></span><br><span class="line">filename = <span class="string">'classification-demo.png'</span></span><br><span class="line"><span class="comment"># you may modify it to switch to another model. The name is case-insensitive</span></span><br><span class="line">model_name = <span class="string">'ResNet50_v1d'</span></span><br><span class="line"><span class="comment"># download and load the pre-trained model</span></span><br><span class="line">net = gluoncv.model_zoo.get_model(model_name, pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># load image</span></span><br><span class="line">img = mx.image.imread(filename)</span><br><span class="line"><span class="comment"># apply default data preprocessing</span></span><br><span class="line">transformed_img = gluoncv.data.transforms.presets.imagenet.transform_eval(img)</span><br><span class="line"><span class="comment"># run forward pass to obtain the predicted score for each class</span></span><br><span class="line">pred = net(transformed_img)</span><br><span class="line"><span class="comment"># map predicted values to probability by softmax</span></span><br><span class="line">prob = mx.nd.softmax(pred)[<span class="number">0</span>].asnumpy()</span><br><span class="line"><span class="comment"># find the 5 class indices with the highest score</span></span><br><span class="line">ind = mx.nd.topk(pred, k=<span class="number">5</span>)[<span class="number">0</span>].astype(<span class="string">'int'</span>).asnumpy().tolist()</span><br><span class="line"><span class="comment"># print the class name and predicted probability</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'The input picture is classified to be'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'- [%s], with probability %.3f.'</span>%(net.classes[ind[i]], prob[ind[i]]))</span><br></pre></td></tr></tbody></table></figure>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/05/27/tricks-of-train/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/01/06/swin/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/06/swin/" class="post-title-link" itemprop="url">Swin</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-06 15:25:57" itemprop="dateCreated datePublished" datetime="2022-01-06T15:25:57+08:00">2022-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-20 12:03:28" itemprop="dateModified" datetime="2022-03-20T12:03:28+08:00">2022-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Vision-Transformer/" itemprop="url" rel="index">
                    <span itemprop="name">Vision Transformer</span>
                  </a>
                </span>
            </span>

          
            <span id="/2022/01/06/swin/" class="post-meta-item leancloud_visitors" data-flag-title="Swin" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/06/swin/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/06/swin/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><blockquote>
<p>swin transformer是对vision transformer的改进版，主要思想在于在vision transformer的基础上，引入了卷积的归纳偏置，设计了分层的结构，针对特征图，只在windows窗口内进行self-attention的计算，取得了速度与效果的平衡，vision transformer需要比较大的数据来进行模型的训练才能取得比较好的效果，swin transformer在imagenet-1k上面，也能取得很好的效果。</p>
</blockquote>
<hr>
<p>Title: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</p>
<p>Author : Liu Ze, Hu Han</p>
<p>微软亚洲研究院</p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>视觉相比NLP的不同点： 图像中的实例变化尺度比较大，并且图像的分辨率很高。==这也是本文主要解决的两个创新点，问题一解决方式是分层操作， 问题二解决的方式是window内进行self-attention计算。==</li>
<li>shifted window scheme,通过限制了不重叠的窗口+跨窗口的交互， 极大的提高了效率。</li>
<li>对图像的不同大小的尺寸适应性比较好，（不像vit，尺寸固定），可以适应不同的视觉任务。</li>
<li>image classification : 86.4%</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>不同点总结：</p>
<blockquote>
<p>文字具有固定的尺度的，但是图像中的对象的尺度变换很大。</p>
<p>图像的分辨率很高，比如图像分割，需要在像素级别进行密集预测，这会带来self-attetion计算复杂度的平方增长。</p>
</blockquote>
</li>
</ul>
<ul>
<li><p>swin transformer 分层就是最开始是小尺寸的patch,然后合并相邻的patch使得patch的尺寸变大。</p>
<p><img src="/image-20211019173109571.png" alt="image-20211019173109571"></p>
</li>
<li><p>self-attention是在windows中计算的，一个window中包含的patch的数量是固定的。</p>
</li>
<li><p>swin transformer的一个最大的创新点是 shift window，</p>
</li>
<li><p><img src="/image-20211018193249301.png" alt="image-20211018193249301"></p>
</li>
</ul>
<h2 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h2><ul>
<li>近期的工作中，有一些是将self-attention应用到局部窗口(slide window)中，对每个像素进行计算attention==[32,49,77]这个是怎么做的可以了解一下==;虽然他们取得了加好的acc/flops 的trade off， 但是由于其频繁的内存访问，导致其延时还是比较高的。==本文使用shift windows,取得更高的效率==</li>
<li>vit不错，但是vit不适用于密集视觉任务，以及对尺寸不友好。</li>
<li>==The number of patches in each window is fixed==, and thus the complexity becomes linearto image size.</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li>all arch:  </li>
<li>提取每个patch的大小为4x4,这样，每个patch的dim就是4x4x3=48， 然后通过一个linear映射到embedding dim</li>
<li>以224为例子，每张图像提取56x56个patch</li>
<li>hierarchical representation: 通过patch merging layer, 将2x2的相邻patch进行concat合并,然后通过linear 映射将4C的维度降到2C</li>
</ul>
<p><img src="/image-20211019150052112.png" alt="image-20211019150052112"></p>
<ul>
<li>multi-head-att的不同，主要是swin引入了shift window的思想，其他基本都是相同的。</li>
</ul>
<h3 id="shifted-window-based-self-attention"><a href="#shifted-window-based-self-attention" class="headerlink" title="shifted window based self-attention"></a>shifted window based self-attention</h3><ul>
<li>如果不使用窗口，使用全局的attention,对于密集型的预测，计算量会比较大。如下公式是使用window后的计算结果对比，不使用window，跟hw是平方关系，使用window后跟hw是线性关系。</li>
</ul>
<p><img src="/image-20211019175254915.png" alt="image-20211019175254915"></p>
<ul>
<li>swin选择在windows中计算self-attention 每个windows包含多个patches.</li>
<li>但是在window中计算attention也存在一个问题，就是缺乏window间的交互，所以作者提出了如下的shift window方法。</li>
<li>但是上面的方法依然存在问题，在window shift之后，window的数量会增加，因为出现了不满足窗口大小的的windows. 这里作者提出了cyclic-shifting 方法。如下图，函数就是==torch.roll实现==，这样在计算atten的时候，将移动的地方mask掉就可以了，不影响计算量。 ==shift是交替进行的，这里如果我没有理解错，就是shift windows内patch数量的一半，win内patch的数量默认为7，这样随着层数加深的merge操作，shift的越来越多，可以理解为感受野越来越大==</li>
<li>在代码中，这种交互在不同的block中是交替进行的。</li>
</ul>
<p><img src="/image-20211019153516460.png" alt="image-20211019153516460"></p>
<p><img src="/image-20211019154534335.png" alt="image-20211019154534335"></p>
<ul>
<li>本文也是因为使用了分层操作，使得一些密集操作可以实现，为啥可以实现呢？因为通过分层合并，patch的数量有效的减少了，计算量也就有效的减少了。</li>
</ul>
<h2 id="exp"><a href="#exp" class="headerlink" title="exp"></a>exp</h2><ul>
<li>train 参数： lr 0.001, adamw， cosine decay, 300 epoch, bs 1024, weight decay 0.05</li>
<li>finetune 参数： 10e-5, 30 epoch, weight decay 10e-8</li>
<li>大量的augment的应用</li>
</ul>
<hr>
<p>附录：</p>
<p><strong>swin-transformer, 维度运算总结</strong></p>
<p>输入图像： (1,3,224,224)</p>
<p>merge维度，4x4大小的patch：（1,56,56,96）,先不考虑batch 就是  （56,56,96）</p>
<p>提取窗口，7x7个patch组成一个窗口，  （64,7,7,96），合并窗口维度后 =》 （64,49,96）   ### 如果输入的batchsize不是1，改为2，这里就变成了 （128,49,96）</p>
<p>计算QKV就是一个linear,  self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)</p>
<p>Q, K, V : (64,3,49,32) , 如何来的呢？ QKV一起的:reshape(B, N，3， num_head, C//num_head).permute(2, 0, 3, 1, 4)， 然后QKV[0], QKV[1], QKV[2]</p>
<p>atte.shape = (64, 3, 49, 49)</p>
<p>经过attention运算，输出的维度为： （64 49,96）</p>
<p>经过一个fc进行维度的调整，输出： （64,49,96）</p>
<p>shift用 torch.roll实现</p>
<p><img src="/clipboard.png" alt="img"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/367111046">https://zhuanlan.zhihu.com/p/367111046</a></p>
<p>==================================下面是nlp中transformer的维度分析</p>
<p>输入： [batch, len, dim ]</p>
<p>Q, K, V  [batch, head, len, dim/head ]</p>
<p>attn : [batch , head, len, len]</p>
<p>经过attention输出， [batch , len, dim]</p>
<p>经过fc进行维度调整后依然是， [batch, len, dim]</p>
<p>总结; 其实总结后会发现，这两个其实是一样的套路。</p>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/06/swin/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2022/01/06/vit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/06/vit/" class="post-title-link" itemprop="url">ViT</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-06 15:25:57" itemprop="dateCreated datePublished" datetime="2022-01-06T15:25:57+08:00">2022-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-28 08:17:16" itemprop="dateModified" datetime="2022-01-28T08:17:16+08:00">2022-01-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Vision-Transformer/" itemprop="url" rel="index">
                    <span itemprop="name">Vision Transformer</span>
                  </a>
                </span>
            </span>

          
            <span id="/2022/01/06/vit/" class="post-meta-item leancloud_visitors" data-flag-title="ViT" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/06/vit/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/06/vit/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><h1 id="AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><blockquote>
<p>这是一个比较有代表性的，将transformer引入cv的一个例子，其也没有特别的想法，主要创新点就是通过对图像进行patch的提取，进而构建了类似NLP的任务，然后将bert代码搬过来，进行图像的分类。</p>
</blockquote>
<hr>
<p>Author: Alexey Dosovitskiy</p>
<p>Google Brain</p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks</li>
<li>When ==pre-trained on large amounts of data== and ==transferred to multiple mid-sized or small image recognition benchmarks== (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub-stantially fewer computational resources to train.（abstract这里强调了大量的数据，说明大量的数据对于transformer是很必要的）</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>we ==split an image into patches== and provide the sequence of linear embeddings of these patches as an input to a Transformer.</li>
<li>Image patches are treated the same way as ==tokens (words)== in an NLP application.</li>
<li>同样的结论，self-Atten在中小规模数据表现略差，主要原因是self-atten的归纳能力不如cnn那么好，CoAtNet估计就是借鉴的这里，大规模数据可以解决这个问题。</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li>NLP是2维，图像是3维，那么如何将图像像NLP迁移呢，vit通过patch进行对齐，对 H W C的图像，先取N个patch，尺寸就变成 N*(P*P*C)，然后将(P*P*C)，利用fc映射到一定的dim，就变成了 N * Dim，就变成了二维矩阵，就可以直接使用NLP的transformer进行训练了。想法真的挺简单的。</li>
<li>引入了[cls]token，==文中说这个是 learnable embedding，这个是咋实现的？看过代码后知道了，其实就是nn.Parameter,构造了一个可学习的参数插入到每张图像patch前面==</li>
<li>引入了postion embedding, 使用的是一维的embedding,（作者实验发现，二维并没有明显的效果）, learnable</li>
</ul>
<p><img src="/image-20210930100559146.png" alt="image-20210930100559146"></p>
<ul>
<li>hybrid : 除了选择原始的图像进行patch的提取，也可以在cnn得到的feature map上面提取patch，特别的，patch的大小就是1x1(后面的实验部分作者就是用的1x1)</li>
<li>finetune问题，通常训练好pre-trained model后，通常在higher resolution上会有更好的效果，可是这里会遇到一个问题，当提高resolution之后，如果依然采用原来的patch的大小，那么seq的长度就会增加，之前训练的pos embedding就失效了，这里作者采用了一个2D插值方法，利用原有的位置进行插值，得到新的位置embedding.</li>
</ul>
<p><img src="/image-20210930143324226.png" alt="image-20210930143324226"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul>
<li>train setting: batch_size:4096;  adam $\beta_1=0.1,\beta_2=0.999$， dropout applied after ever dense layer</li>
<li>这里设计了3款vit,分别是 base ,large,huge, 可以看到 Huge的优势还是有一些的。</li>
</ul>
<p><img src="/image-20210930105538209.png" alt="image-20210930105538209"></p>
<ul>
<li>==那么多大的数据量是需要的呢？==做了个实验，实验都是先pretrained，再在imagenet上面finetune,在imagenet小数据集上面相比conv，transformer不行，随着数据量的增加，transformer的爆发力逐渐发挥出来。</li>
<li><img src="/image-20210930110700342.png" alt="image-20210930110700342"></li>
</ul>
<ul>
<li>This result reinforces the intuition that the==convolutional inductive bias====  is useful for smaller datasets, but for larger ones, learning the ==relevant patterns== is sufficient,even beneficial. </li>
<li>Scaling study，对于小模型，采用hybrid方式， 实验证明是有优势的，模型大了几乎就相近了。</li>
<li>self-supervision: 作者同样做了类似bert的mask的无监督实验，结果显示，在JFT数据做无监督后，会比imagenet上的pretrain高两个点，但是比在JFT有监督训练还是低了几个点。</li>
<li>Adam相比SGD要好一些，但是也没有好那么多。</li>
</ul>
<h2 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码:"></a>关键代码:</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">self.to_patch_embedding = nn.Sequential(</span><br><span class="line">    Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1 = patch_height, p2 =patch_width),</span><br><span class="line">            nn.Linear(patch_dim, dim),)</span><br><span class="line">            </span><br><span class="line">self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))</span><br><span class="line">self.cls_token = nn.Parameter(torch.randn(1, 1, dim))</span><br><span class="line">self.mlp_head = nn.Sequential(nn.LayerNorm(dim),nn.Linear(dim, num_classes))</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这是一个比较有代表性的，将transformer引入cv的一个例子，其也没有特别的想法，主要创新点就是通过对图像进行patch的提取，进而构建了类似NLP的任务，然后将bert代码搬过来，进行图像的分类。</p>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/06/vit/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2021/11/06/CoAtNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/06/CoAtNet/" class="post-title-link" itemprop="url">CoAtNet</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-06 15:25:57" itemprop="dateCreated datePublished" datetime="2021-11-06T15:25:57+08:00">2021-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-28 03:45:56" itemprop="dateModified" datetime="2022-01-28T03:45:56+08:00">2022-01-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Vision-Transformer/" itemprop="url" rel="index">
                    <span itemprop="name">Vision Transformer</span>
                  </a>
                </span>
            </span>

          
            <span id="/2021/11/06/CoAtNet/" class="post-meta-item leancloud_visitors" data-flag-title="CoAtNet" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/11/06/CoAtNet/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/11/06/CoAtNet/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><blockquote>
<p>这篇文章更偏向于网络设计，主要在于将conv与atten相结合，各取其有点，conv更擅长进行归纳总结，atten具有更大的容量，所以将二者进行结合，可以去得进一步提点的效果，然后为了减少计算量，采用了前面是conv，后面是atten的结构，同时，实验了结构里面每个stage有多少的block效果最好等等，代码没有开源，具体的实现细节还需要等开源再看。</p>
</blockquote>
<hr>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04803v2.pdf">https://arxiv.org/pdf/2106.04803v2.pdf</a></p>
<p>Google Brain</p>
<p>Zihang Dai</p>
<p>没开源</p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>虽然transformers倾向于更大的模型容量，但是由于其缺乏正确的归纳偏差（right inductive bias），可能会比conv弱一些。==这里后来我的理解就是泛化性比较弱，所以需要大量的图像嘛==。</li>
<li>depthwise conv + self-attention</li>
<li>vertical stacking conv and attention</li>
<li>imagenet : 86%,  if use jft-3b : 99.88%</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>vit的better performance 很大程度上是依赖于大量的数据的JFT-300，如果没有大量数据，vit效果不行的。</li>
<li>说缺乏certain desireable inductive biases, 需要大量的数据进行补偿，==归纳偏差是说对图像的特征进行归纳吗？没太理解？==， 后来在wikipedia找到了答案：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Inductive_bias%EF%BC%8C%E7%AE%80%E5%8D%95%E8%AF%B4%E5%B0%B1%E6%98%AF%E5%BD%92%E7%BA%B3%E8%83%BD%E5%8A%9B%EF%BC%8C%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E3%80%82">https://en.wikipedia.org/wiki/Inductive_bias，简单说就是归纳能力，泛化能力。</a></li>
<li>卷积具有较强的归纳能力，可以快速收敛；atten具有较好的模型能力，适应较大的数据量，那将二者进行结合不就既有了generalization又有了model capacity.</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="Merging-Convolution-and-Self-Attention"><a href="#Merging-Convolution-and-Self-Attention" class="headerlink" title="Merging Convolution and Self-Attention"></a>Merging Convolution and Self-Attention</h3><ul>
<li><p>选择mobilnet里面的depth wise conv作为conv block； 因为mobilenet和transformer的ffn都是先提高4倍维度，然后又降回ori</p>
</li>
<li><p>conv和self attention,一样都是针对一块区域进行处理， conv是针对fix size进行处理比如3x3, self attention是针对全图进行处理。</p>
<blockquote>
<p>对比分析：</p>
<ul>
<li><p>conv的kernel是一个独立的超参数，而self attention是根据输入决定的，所以self attention更适合处理复杂的空间位置关系，这个属性比较适合处理high-level的信息，但是也比较容易过拟合。</p>
</li>
<li><p>==对于给定的position pair(i, j), conv weight 更关注与i 和 j 的偏移信息，而不是i和j的值。这个属性通常被称为等价翻译，更使用于提高泛化性。而由于vit使用了绝对的位置信息，破坏了这个属性，所以其效果变差。==这个可以好好分析分析，不是特别的理解。</p>
</li>
<li><p>感受野问题，self attention 具有大的感受野（但是大的感受野，往往带来的是较大的计算量。）</p>
</li>
</ul>
</blockquote>
</li>
<li><p>基于上面的分析，很自然的想法就是如下的结合：$w_{i-1}$是一个常数（==咋算的后面附录A.1详述==）</p>
<p><img src="/image-20210928114232584.png" alt="image-20210928114232584"></p>
</li>
</ul>
<h3 id="Vertical-Layout-Design"><a href="#Vertical-Layout-Design" class="headerlink" title="Vertical Layout Design"></a>Vertical Layout Design</h3><ul>
<li><p>如果按照上面的结构进行设计，由于attention是对全图进行计算，那么计算量会比较大，有啥方法可以尝试减少计算量呢？</p>
<blockquote>
<ul>
<li>先降维，然后再使用attention操作。（==作者的主要尝试与探索==）</li>
<li>限制感受野，像卷积那样，使用小的感受野。（==发现其实没有用，增加了访存次数，反而更慢了==）</li>
<li>softmax attention 替换成近似的linear attention, 减少计算量。（==试了，没有啥效果==）</li>
</ul>
</blockquote>
</li>
<li><p>那么具体应该怎么设计呢？作者进行了实验</p>
<blockquote>
<ul>
<li>作者进行了5个模式的设计，分别为：VIT，CCCC,CCCT,CCTT,CTTT， C代表conv, T代表transformer, C在T的前头。</li>
<li>结论： 泛化性： CCCC ~~ CCCT  &gt; CCTT  &gt; CTTT  &gt;&gt;  VIT</li>
<li>结论： 模型容量： CCTT~~ CTTT  &gt; VIT  &gt; CCCT &gt;  CCCC</li>
<li>实验结果说明，CCTT和CCCT表现差不多，最终作者进行了实验，根据迁移能力进行选择，最终CCTT胜出。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul>
<li>作者在imagenet-1k, imagenet-21k, JFT上进行实验。 we first pre-train our models on each of the three datasets at resolution 224 for 300, 90 and 14 epochs respectively. Then, we finetune the pre-trained models on ImageNet-1K at the desired resolutions for 30 epochs and obtain the corresponding evaluation accuracy.</li>
<li>RandAugment and MixUp + stochastic depth  ， label smoothing  and weight decay,==又有了这个stochastic depth==</li>
<li>Specifically, we have an interesting observation that if a certain type of augmentation is entirely disabled during pre-training, simply turning it on during fine-tuning would most likely harm the performance rather than improving==这个观察很神奇？why?==</li>
<li>实验效果，论文中表示，效果挺好的。</li>
</ul>
<h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><ul>
<li>加入attention的作用</li>
</ul>
<p><img src="/image-20210929143259557.png" alt="image-20210929143259557"></p>
<h2 id="model-Details"><a href="#model-Details" class="headerlink" title="model Details"></a>model Details</h2><p><img src="/image-20210929143943118.png" alt="image-20210929143943118"></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>A.1， 公式（3）的实现，那个w是如何实现的呢？我也是初略的看了，不知道对不对，作者设了一个（2H-1）x (2W-1)的参数，然后计算的时候直接进行索引。</p>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/06/CoAtNet/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2021/11/06/Cait/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/06/Cait/" class="post-title-link" itemprop="url">Cait</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-06 15:15:14" itemprop="dateCreated datePublished" datetime="2021-11-06T15:15:14+08:00">2021-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-20 10:20:26" itemprop="dateModified" datetime="2022-03-20T10:20:26+08:00">2022-03-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Vision-Transformer/" itemprop="url" rel="index">
                    <span itemprop="name">Vision Transformer</span>
                  </a>
                </span>
            </span>

          
            <span id="/2021/11/06/Cait/" class="post-meta-item leancloud_visitors" data-flag-title="Cait" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/11/06/Cait/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/11/06/Cait/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><h1 id="Cait-Going-deeper-with-Image-Transformers"><a href="#Cait-Going-deeper-with-Image-Transformers" class="headerlink" title="Cait: Going deeper with Image Transformers"></a>Cait: Going deeper with Image Transformers</h1><blockquote>
<p>本文是对vision transformer的改进，主要贡献在于layerScale以及class-attention这两点，最近facebook研究transformer的那伙人发了好几篇针对transformer的修改篇，我觉的这篇有点不太实用，没有进行具体的尝试。</p>
</blockquote>
<hr>
<p>author: Facebook AI</p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>主要探讨模型结构和优化的关系</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>LayerScale : 有利于加速收敛以及改善精度，当transformer较深的时候。</li>
<li>class-attention</li>
<li>迁移学习效果很不错</li>
</ul>
<h2 id="Deeper-image-transformers-with-LayerScale"><a href="#Deeper-image-transformers-with-LayerScale" class="headerlink" title="Deeper image transformers with LayerScale"></a>Deeper image transformers with LayerScale</h2><ul>
<li>goal: 在将transformer迁移到视觉领域的时候，可以训练的更深，更稳定。</li>
<li>DeiT训练了12层，但是事实证明不能训练的更深了。</li>
<li>==vit和Deit都是pre norm 模型包括swin transformer也是pre norm， 传统的transformer是post-norm 模型。==<em>这倒是个以前没有注意到的点。</em> 这个作者做了实验，在deit上，如果采用post-norm 会发现不收敛。</li>
<li>Fixup, ReZero, skipInit，这几个方法都是在residual blocks的输出上，加上了一个==可学习的常数==，然后移除了pre-norm。</li>
<li>作者在实验中发现，移除norm采用fix那几个方法并不奏效，反而训练很不稳定，但是把norm加回来训练就稳定多了，所以作者就加回来了，还做了改进，由单个scalar变成了一个对角矩阵。</li>
<li>layerscale的参数都设置的比较小，前18层是0.1 ，然后是10e-5,然后24层是10e-6</li>
</ul>
<p><img src="/image-20211021160222487.png" alt="image-20211021160222487"></p>
<h2 id="Specializing-layers-for-class-atention"><a href="#Specializing-layers-for-class-atention" class="headerlink" title="Specializing layers for class atention"></a>Specializing layers for class atention</h2><ul>
<li>This design aims at circumventing one of the problems of the ViT architecture: the learned weights are asked to optimize two contradictory(矛盾) objectives:(1) guiding the self-attention between patches while (2) summarizing the information useful to the linear classifier.==很重要的一句话，说出来作者的idea来源==</li>
<li>改变了cls的位置，不是在一开始就插入，使得最开始的layer可完全在patch中计算self-attention。</li>
<li>self-attention 跟vit差不多，不过没有了[cls]。</li>
<li>class-attention  将patch embedding合成到cls中进行分类。</li>
</ul>
<p><img src="/image-20211021170138217.png" alt="image-20211021170138217"></p>
<ul>
<li><p>class-attention是怎么做的呢？主要就是下面这个公式，理解这个公式就可以里，其中 Q是只有xclass的，z是[xclass, xpatch]</p>
<p><img src="/image-20211021172133697.png" alt="image-20211021172133697"></p>
</li>
<li><p>这里说一下计算$Q*K^T$本来是len_q * dim  和  len_k * dim  ,这里的len_q 就变成了1， 最后得到的attention也变成了 1 x dim</p>
</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul>
<li><p>vit训到18层就训不下去了，24层反而效果变得比较差了，特别是只有imagenet数据的时候。</p>
</li>
<li><p>加了layerscale可以使得训练的更稳定，layer间的差别更小，下图 右边。</p>
</li>
<li><p><img src="/image-20211021180832794.png" alt="image-20211021180832794"></p>
</li>
<li><p>AdamW + cosine + 5 warmup + weight decay 0.05 </p>
</li>
<li><p>引入了Deit的蒸馏策略，采用”hard distillation”</p>
</li>
<li><p>Deti -&gt;  Cait的优化进程：</p>
<p><img src="/image-20211022112141837.png" alt="image-20211022112141837"></p>
</li>
</ul>
<ul>
<li>conv时候采用center-crop(256x256中间抠出来224x224)，但是wightman发现其实不crop对于transformer更加的友好，上面的表格中调整crop ratio的比例后，也涨了一点点。</li>
</ul>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/06/Cait/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2021/11/06/yolox/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/06/yolox/" class="post-title-link" itemprop="url">YOLOX</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-11-06 15:04:55 / 修改时间：19:12:00" itemprop="dateCreated datePublished" datetime="2021-11-06T15:04:55+08:00">2021-11-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Detection/" itemprop="url" rel="index">
                    <span itemprop="name">Detection</span>
                  </a>
                </span>
            </span>

          
            <span id="/2021/11/06/yolox/" class="post-meta-item leancloud_visitors" data-flag-title="YOLOX" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/11/06/yolox/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/11/06/yolox/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><ul>
<li>Auther:  Zheng Ge</li>
<li>Github： <a target="_blank" rel="noopener" href="https://github.com/Megvii-BaseDetection/YOLOX">https://github.com/Megvii-BaseDetection/YOLOX</a>.</li>
<li>作者解读： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/p4Porn9KayizQiQIzTTFKA">https://mp.weixin.qq.com/s/p4Porn9KayizQiQIzTTFKA</a></li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>引入anchor-free</li>
<li>decoupled head ,simOTA</li>
<li>Performance:  yolo-nano:  1.8% ap； yolov3 : 3%; YOLOv5-L:1.8% ; </li>
<li>provide tensorrt onnx version</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>近两年目标检测的研究热点： anchor-free； advanced label assignment strategies[37,36,12,41,22,4]; NMS-free detectors[2,32,39]</li>
<li>YOLOv3 as default</li>
<li>we boost the YOLOv3 to 47.3%AP (YOLOX-DarkNet53) on COCO with 640 × 640 resolution, surpassing the current best practice of YOLOv3(44.3% AP, ultralytics version2) by a large margin.’</li>
<li>YOLOv5 640x640； 50.0% AP, supass 1.8%AP</li>
<li></li>
</ul>
<h2 id="YOLOX"><a href="#YOLOX" class="headerlink" title="YOLOX"></a>YOLOX</h2><h3 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h3><ul>
<li>300 epoch , 5 epoch warmup</li>
<li>lr = init_lr * (batchsize/64),    init_lr = 0.01,   cos lr schedule,  (8-gpu, batchsize:128)</li>
<li>input size : 448 to 832 , 32 strides</li>
</ul>
<h3 id="YOLOv3-baseline"><a href="#YOLOv3-baseline" class="headerlink" title="YOLOv3 baseline"></a>YOLOv3 baseline</h3><ul>
<li>DarkNet53 +  SPPlayer</li>
<li>==adding EMA weights updating, cosine lr schedule, IoU loss and IoU-aware branch.  We use BCE Loss for training cls and obj branch, and IoU Loss for training reg branch.==</li>
</ul>
<h3 id="Decoupled-head"><a href="#Decoupled-head" class="headerlink" title="Decoupled head"></a>Decoupled head</h3><p><img src="/image-20210918121226730.png" alt="image-20210918121226730"></p>
<p><img src="/image-20210918121247956.png" alt="image-20210918121247956"></p>
<h3 id="Strong-data-augmentzation"><a href="#Strong-data-augmentzation" class="headerlink" title="Strong data augmentzation"></a>Strong data augmentzation</h3><ul>
<li>Mosaic and Mixup, and closed it for the last 15 epoches</li>
<li>After using strong data augmentation, ImageNet pre-trained is no more beneficial , == train all the following models from scratch.==</li>
</ul>
<h3 id="Anchor-free"><a href="#Anchor-free" class="headerlink" title="Anchor-free"></a>Anchor-free</h3><h3 id="Multi-positives"><a href="#Multi-positives" class="headerlink" title="Multi positives"></a>Multi positives</h3><ul>
<li>not only the center point as positive ; center 3x3 area as positives ; as ‘center sampling’ in Fcos, balance the positive/negative samplings.</li>
</ul>
<h3 id="SimOTA："><a href="#SimOTA：" class="headerlink" title="SimOTA："></a>SimOTA：</h3><ul>
<li>==Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. In CVPR, 2021.==</li>
<li>simOTA是在上面基础上的优化，去掉了Sinkhorn-Knopp  算法，选择topk替代。</li>
<li>OTA的主要作用是，对训练的样本的动态分配，可以在训练的时候，自动计算正样本；可参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/394392992">https://zhuanlan.zhihu.com/p/394392992</a></li>
</ul>
<h3 id="aug的提升效果："><a href="#aug的提升效果：" class="headerlink" title="aug的提升效果："></a>aug的提升效果：</h3><p><img src="/image-20210922111546338.png" alt="image-20210922111546338"></p>
<h2 id="Other-backbone"><a href="#Other-backbone" class="headerlink" title="Other backbone"></a>Other backbone</h2><h3 id="vs-yolov5"><a href="#vs-yolov5" class="headerlink" title="vs yolov5"></a>vs yolov5</h3><ul>
<li>use cspnet , silu activation , pan head</li>
</ul>
<p><img src="/image-20210922114054103.png" alt="image-20210922114054103"></p>
<h3 id="vs-tiny-nano-detector"><a href="#vs-tiny-nano-detector" class="headerlink" title="vs tiny, nano detector"></a>vs tiny, nano detector</h3><p><img src="/image-20210922114234135.png" alt="image-20210922114234135"></p>
<h3 id="model-size-and-data-augmentation"><a href="#model-size-and-data-augmentation" class="headerlink" title="model size and data augmentation"></a>model size and data augmentation</h3><ul>
<li>对于小模型，需要减少augment(mosaic,mixup等)；对于大模型需要增强augment</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>本文依然是在yolov3的基础上进行一系列的魔改，增加了anchor free；decouple head; augments等，最终提升了模型的效果，在大模型和小模型中，都取得了一定的优势。</li>
</ul>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/06/yolox/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wanglichun.tech/2021/11/06/albert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.png">
      <meta itemprop="name" content="Spring Wang">
      <meta itemprop="description" content="众里寻他千百度，蓦然回首，那人却在灯火阑珊处">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Spring's Idea">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/06/albert/" class="post-title-link" itemprop="url">Albert,A Lite Bert</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-11-06 14:41:47 / 修改时间：19:03:04" itemprop="dateCreated datePublished" datetime="2021-11-06T14:41:47+08:00">2021-11-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
            <span id="/2021/11/06/albert/" class="post-meta-item leancloud_visitors" data-flag-title="Albert,A Lite Bert" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/11/06/albert/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/11/06/albert/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head></head><body></body></html><html><head></head><body><h1 id="Albert-A-Lite-BERT"><a href="#Albert-A-Lite-BERT" class="headerlink" title="Albert: A Lite BERT"></a>Albert: A Lite BERT</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>use a self-supervise loss to focus on modeling inter-sentence coherence</li>
<li><a target="_blank" rel="noopener" href="https://github.com/google-research/ALBERT">https://github.com/google-research/ALBERT</a>.</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>It has become common practice to pre-train large models and distill them down to smaller ones</li>
<li>==Albert two parameter reduction techniques:==   (1) factorized embedding parameterization, 通过将vocabularyembedding matrix进行分解，分解成2个小的matrix，这使得可以使用更大的hidden size   (2) cross-layer parameter sharing,这使得模型不会随着网络的加深而参数量增加</li>
<li>albert 可以减少bert 18倍的参数量，提升1.7倍的训练速度</li>
<li>self-supervised loss for sentence-order perdiction:  defined on textual segments rather than sentences</li>
</ul>
<h2 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h2><ul>
<li>跟bert设置 一致： 使用gelu；feed-forward/filter: 4H;    attention heads: H/64</li>
<li>wordpiece: bert等都有用到，就是根据词根进行提取token，比如loved,loving等，lov + ed  + ing</li>
<li>embedding的映射，默认是到hidden size维度的，这样会存在一个较大的矩阵，albert采用的方法是，对齐进行分解，先映射到E维然后再映射到H维度，计算量从$V*`H -&gt;  V<em>E+E</em>H,H &gt;&gt; E$</li>
<li>parameter sharing; 实验发现，这样使得不同层之间的参数更稳定</li>
<li>NSP loss被发现并不是很可靠，猜测可能是这个任务太简单了，所以本文采用了行的SOP(sentence-order prediction) loss，简单来说就是将连续的句子前后顺序倒过来，作为负样本进行预测。</li>
<li>tokenized using SentencePiece as in XLNet。</li>
<li>n-gram mask;  不只是mask词，还mask短语。</li>
</ul>
<h2 id="Exp"><a href="#Exp" class="headerlink" title="Exp"></a>Exp</h2><ul>
<li><p>embedding size: E , 128效果较好。</p>
<p><img src="/image-20210827143705772.png" alt="image-20210827143705772"></p>
</li>
<li><p>cross layer parameter sharing。</p>
<p>事实证明，share attention的效果是最好的，但是相差也没有很大，所以albert默认是参数全部共享的。</p>
<p><img src="/image-20210827144640102.png" alt="image-20210827144640102"></p>
</li>
</ul>
</body></html>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/06/albert/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Spring Wang"
    src="/images/head.png">
  <p class="site-author-name" itemprop="name">Spring Wang</p>
  <div class="site-description" itemprop="description">众里寻他千百度，蓦然回首，那人却在灯火阑珊处</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lichun-wang" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;lichun-wang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/lichun_wang1993@163.com" title="E-Mail &amp;rarr; lichun_wang1993@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Chunfengyanyulove" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;Chunfengyanyulove" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/chunfengyanyu" title="Weibo &amp;rarr; https:&#x2F;&#x2F;weibo.com&#x2F;chunfengyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Spring Wang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v6.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        






  <script>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz',
            'X-LC-Key': 'VAwpszUEpH7osEG3O76jh3be',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>






        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'Lgw7TumxtycA4MCC8Ro8gL4a-gzGzoHsz',
    appKey: 'VAwpszUEpH7osEG3O76jh3be',
    placeholder: "comments",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
